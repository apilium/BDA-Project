---
title: "Heart Failure Prediction using Bayesian Approach"
author: "Teemu Sormunen, Abdullah GÃ¼nay, Nicola Brazzale"
date: "11/18/2020"
header-includes:
  - \usepackage[numbers]{natbib}
# Citations
bibliography: ./citations/sources.bib
# csl: ./citations/vancouver-brackets.csl unnecessary?

output: 
  pdf_document:
    toc: true
    toc_depth: 2
    number_sections: true
    citation_package: natbib

---

\newpage

# Introduction

In this project we analyse the data from Heart disease dataset [@origpaper]. We perform a Bayesian analysis of the data using this sequence of operations:

- **Overview of analysis problem and of the dataset**
- **Data preprocessing and visualisation**
- **Prior choice discussion**
- **Models used in our analysis**
- **$\hat{R}$ convergence**
- **HMC specific convergence diagnostics**
- **Effective sample size diagnostic (n_eff or ESS)**
- **Model comparison**
- **Prior sensitivity analysis**
- **Discussion and conclusion**

In our study, we are observing the dataset from two distinct point of views. The first approach was based on the simple predictive analysis whether patient dies, or survives  the research follow up period (4 - 285 days), and on the other hand we want to create an actual survival analysis, which takes time and censored variables in to consideration. Different models are used to carry out this analysis and a model comparison is perfomred in order to asses the quality of the outcome. More informations in upcoming chapters. 

## The problem
Cardiovascular Heart Disease (CHD) is the top reason causing 31% of deaths globally. Pakistan is one of the countries where CHD is increasing significantly, and previous studies do not directly apply to Pakistani area due to different diet patterns.
[@origpaper]

## The motivation
With this project we aim to estimate death events and the major risk factors for heart failure with, possibly, high accuracy [@origpaper].

## Modeling idea

We created 4 models which are then compared based on $\hat{r}$, $n_{eff}$, using the loo package and the classification accuracy. The three first models ignore the time feature, and simply predict whether patient has died (1) during the experiment duration, or survived (0). We chose this approach to practise survival analysis with binary outcome.  
We also created fourth model, which predicts the time with respect to death event. In this case, the death event 1 means the patient has died, and death event 0 means that the patient is censored from the study. Censoring practically means, that the patient has opted out of the study, and the researches couldn't reach him anymore. In this context, it doesn't necessarily mean that the patient has survived, but we just don't know the outcome.  

The 1st model is the reduced model and consists in fewer varibles which are selected base on their correlation with the death event. The 2nd model consists in all variables except for the variable "time" as we believe that doesn't represent an important factor in the death event scenario. The 3rd model used is a hierarchical model where we treated age class patients in a group with respect to the other selected variables. 

The 4th model is similar type of linear model, but this time we consider time as outcome variable with respect to death event. We use correlation matrix to select the strongest variable that correspond with time, and we also use BRMS internal function cens() for taking the censored variable DEATH_EVENT to consideration. More clear explanation is given later.  

# Dataset

## Term explanation  

Some of the terms in the dataset might not be familiar, and they are opened briefly here.

* **Creatine phosphokinase (CPK)**  
CPK is an enzyme, which helps to regulate the concentration of adenosine triphosphate (ATP) in cells. ATP is responsible for carrying energy. If the CPK level is high, it often means that there has been an injury or stress on a muscle tissue. Although CPK is one the oldest markers of heart attack, high CPK might also indicate of acute muscle injury along with acute heart problems.  
Normal level of CPK ranges from 20 to 200 IU/L [@phosphokinase]

* **Ejection fraction (EF)**  
EF is a measurement in percentage which describes how much blood left ventricle pumps out of heart with each contraction.
Low EF might indicate potential heart issues.  
Normal EF is 50 to 70 percent, while measurement under 40 percept might be an indicator of heart failure or cardiomyopathy. [@ef]  

* **Platelets**  
Platelets are small cell fragments which can form clots. 
Too many platelets can lead to clotting of blood vessels, 
which in turn can lead to heart attack. Too 
Normal range of platelets is from 150 000 to 450 000. [@platelets]

* **Serum creatinine**  
When creatine breaks down, it forms a waste product called creatinine. Kidneys normally remove creatinine from body. Serum creatinine measures level of creatinine in the blood, indicating the kidney health. High levels of creatinine might indicate a kidney dysfunctioning.  
Normal level of creatinine range from 0.9 to 1.3 mg/dL in men and 0.6 to 1.1 mg/dL in women who are 18 to 60 years old. [@creatinine_serum]

* **Serum sodium**  
Serum sodium measures the amount of sodium in blood. Sodium enters blood through food and drink, and leaves by urine, stool and sweat. Too much sodium can cause blood pressure, while too little sodium can cause nausea, vomiting, exhaustion or dizziness.  
Normal levels of serum sodium are 135 to 145 mEq/L, according to Mayo Clinic. There are however different interpretations of "normal".[@sodium]

* **Time**
Time variable indicates the time since the research has started for that person (the time of ventricular systolic dysfunction). We have time variable included, because we have to inspect when the death events are happening. This variable is ignored in the first three models, because we wanted to also interpret this dataset from binary survival approach, so predict whether patient dies or not.  


## Dataset introduction

The dataset of 299 patients was produced as a result of study [@origpaper] from Pakistani's city Faisalabad. All of the patients were over 40 years old, each having ventricular systolic dysfunction. This means that patient has poor left ventricular ejection fraction. The follow up period was 4 to 285 days, with average of 130 days. This has to be taken in to consideration when doing the survival analysis.  
The dataset has 105 women, and 194 men.
Some features such as: Ejection fraction, serum creatinine and platelets are categorical variables, while age, serum sodium and creatine phosphokinase are continuous variables.  
Statistical analysis by [@origpaper] found age, creatinine, sodium, anemia and BP as significant variables.  
 
\newpage
# Packages
```{r setup, include=FALSE}
library(tidyverse)
library(ggplot2)
library(corrplot)
library(brms)
library(rstanarm)
library(loo)
library(bayesplot)
library(rstan)
```

Load data
```{r}
file.name <- './data/heart_failure_clinical_records_dataset.csv'
heart <- read_csv(file.name)
```
Prevent text overflow on PDF
```{r}
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)
rstan_options(auto_write=TRUE) # Save stan models automatically
```

\newpage
# Data preprocessing and visualization

## Plot histograms

We are first plotting the histograms to get an overview of the dataset.  

It seems like the sex between ages is distributed quite evenly, there's slightly more patients from the 50-60.

```{r}
ggplot(heart, aes(x=age)) + geom_histogram(aes(fill=as.character(sex)), bins = 30) + labs(fill = "Sex")
```

\newpage
This histogram might suggest us that older people die during this follow up period with higher probability, and younger people either survive or opt-out of the study. 
```{r}
ggplot(heart, aes(x=age)) + geom_histogram(aes(fill=as.character(DEATH_EVENT)), bins = 30) + labs(fill = "Death")
```

\newpage
Creatinine phosphokinase doesn't bring too much information based on the histogram, although it shows us that people with high creatinine phosphokinase might tend to die more often. Because almost everyone who attended this study had already increased phosphokinase levels, we should take this interpretation with a slight grain of salt. 
```{r}
ggplot(heart, aes(x=creatinine_phosphokinase)) + geom_histogram(aes(fill=as.character(DEATH_EVENT)), bins = 30) + labs(fill = "Death")
```

\newpage
Ejection fraction seems to correlate strongly with death. This is logical, because ejection fraction measures the hearts ability to pump blood. Here we can again see that most of the people fall under normal levels of EF.
```{r}
ggplot(heart, aes(x=ejection_fraction)) + geom_histogram(aes(fill=as.character(DEATH_EVENT)), bins = 30) + labs(fill = "Death")
```

\newpage
Platelets seems to follow quite even distribution with respect to death event, and there isn't too much information available only based on the histogram.
```{r}
ggplot(heart, aes(x=platelets)) + geom_histogram(aes(fill=as.character(DEATH_EVENT)), bins = 30) + labs(fill = "Death")
```

\newpage
Serum creatinine seems to correlate quite highly with death. When the upper bound for serum creatinine 1.3 is passed, it seems that probability for death becomes high.
```{r}
ggplot(heart, aes(x=serum_creatinine)) + geom_histogram(aes(fill=as.character(DEATH_EVENT)), bins = 30) + labs(fill = "Death")

```

## Correlation matrix

By plotting the correlation matrix we can see the correlations.

```{r}
pred <- c("high_blood_pressure", "age", "sex", "creatinine_phosphokinase", "diabetes", "ejection_fraction", "platelets", "serum_creatinine", "serum_sodium", "smoking", "anaemia", "time") 
target <- c("DEATH_EVENT")
#formula <- paste("DEATH_EVENT ~", paste(pred, collapse = "+"))
p <- length(pred)
n <- nrow(heart)
x = cor(heart[, c(target,pred)]) 
corrplot(x)
```

Looking at the correlations only by eye might not be enough, so let's list the correlations with respect to death_event in sorted order.

```{r}
sort(x[1,])
```

We see that highly positively correlating variables are age and serum creatinine, which found to be true already earlier. Strongest negative correlations are time, ejection fraction and serum sodium. Time clearly indicates that people tend to die early after the ventricular systolic dysfunction has happened. Serum sodium also seems to correlate negatively, because the low sodium levels are usually occurring after heart failure. If we have to make conclusions, then we could conclude that the sodium levels are lower in the people who have had more severe ventricular systolic dysfunction.  

For the fourth survival analysis model we should also look at the highest correlations according to time. 

```{r}
sort(x[nrow(x),])
```

According to time we see that highest negative correlation when death_event is discarded are age, high blood pressure, serum creatinine and anemia. This means that old people tend to die early, with high blood pressure, with high serum creatinine, with anemia. This seems to somewhat run hand in hand with the previous conclusions, although this time high blood pressure and anemia was introduced. High blood pressure sounds like it could lead to heart attack easily, which makes sense. Also anemia seems to go hand in hand with heart attack.  


# Models

We chose to use BRMS for modeling. It stands for Bayesian Regression Models for Stan. It's an interface to fit Bayesian generalized (non-)linear multivariate models using Stan. As we need to do analysis for binary response variables, we need some sort of a generalized linear model. We also chose BRMS due to its ease of use when fitting such complicated multilevel generalized linear models.

In BRMS modeling, the parameters are said to either be population level or group level. Population-level parameters means the same thing as regular parameters in our course, and group-level parameters mean hyperparameters in hierarchical case.  

**Family argument** specifies the distribution family of the response variable.    

**Prior argument** for each of the parameters. One can set different priors for each population level parameter, or group level parameter.

Before creating the test/train datasets, we need to preprocess the data little bit. Ejection fraction is described as percentage, and it can be given prior with beta distribution, which is constrained from 0 to 1. Let's normalize ejection fraction to be from 0 to 1.
```{r}
heart$ejection_fraction = heart$ejection_fraction / 100
```

Create general function for splitting the train and test data.
```{r}
split.train.test <- function(data, test.size = 0.3) {
  train.indice <- sample(nrow(heart), nrow(heart)*(1-test.size))
  train.data <- heart[train.indice,]
  test.data <- heart[-train.indice,]
  return(list("train" = train.data, "test" = test.data))
}
set.seed(1)
new.data <- split.train.test(heart)
train.data <- new.data$train
test.data <- new.data$test
```

## Model fitting

The Generalised Linear Model used for every parametrisation is Bernoulli-Logit Generalised Linear Model for the first three models, which is logistic regression. It's expressed in mathematical terms as following:  

$$BernoulliLogitGLM(y|x,\alpha, \beta) = \prod_{1\leq i\leq n} Bernoulli(y_i | logit^{-1}(\alpha_i + x_i\times \beta))$$


### Full model

Stan code for full model can be found in Appendix A.  
Full model includes all the parameters that are specified in the dataset.

```{r message=FALSE, warning=FALSE}
fit.full <- brm(formula = DEATH_EVENT ~ age + ejection_fraction + serum_creatinine + serum_sodium + high_blood_pressure + creatinine_phosphokinase + diabetes + smoking + anaemia + sex,
                prior = c(set_prior("cauchy(40,20)", class="b", coef="age"),
                          set_prior("inv_gamma(1,5)", class="b", coef="serum_creatinine"),
                          set_prior("beta(6,4)", class="b", coef="ejection_fraction"),
                          set_prior("cauchy(0,200)", class="b", coef="serum_sodium"),
                          set_prior("cauchy(0,4000)", class="b", coef="creatinine_phosphokinase"),
                          set_prior("normal(.5, .5)", class="b", coef="anaemia"),
                          set_prior("normal(.5, .5)", class="b", coef="diabetes"),
                          set_prior("normal(.5, .5)", class="b", coef="smoking"),
                          set_prior("normal(.5, .5)", class="b", coef="high_blood_pressure"),
                          set_prior("normal(.5, .5)", class="b", coef="sex")),
           data = train.data,
           family = bernoulli(),
           refresh=0
)
```

### Feature selected model

Stan code for feature selected model can be found in Appendix B.  

In feature selected model, we hand pick the features that seems to be the most promising with regards to fitting the model. As described above in correlation analysis, we saw that ejection_fraction, serum_creatinine, serum_sodium and age were correlating to death.  

Based on this we can choose these variables to be the important ones, and build a model with them.

```{r message=FALSE, warning=FALSE}
fit.feature_selected <- brm(formula = DEATH_EVENT ~ ejection_fraction + serum_creatinine + serum_sodium + age,
                            data = train.data,
                            family = bernoulli(),
                          prior = c(set_prior("cauchy(40,20)", class="b", coef="age"),
                          set_prior("inv_gamma(1,1)", class="b", coef="serum_creatinine"),
                          set_prior("beta(6,4)", class="b", coef="ejection_fraction"),
                          set_prior("cauchy(0,200)", class="b", coef="serum_sodium")),
                          refresh = 0,
                          control = list(adapt_delta = 0.99)
                            )
```


### Hierarchical model

Stan code for hierarchical model can be found in Appendix C.  

In hierarchical model, we choose age as hyperparameter, because by intuition we thought that different aged people tend to have different medical conditions by default. This intuition is supported by calculating the absolute row sums of the correlation matrix rows, and seeing absolute correlations of variables. 
```{r}
sort(rowSums(abs(x)))
```

First we will discretize age data in to 3 equal depth bins.

```{r}
discretize.variable <- function(variable.to.discretize, thresholds = c(55, 70)) {
  variable.to.discretize[variable.to.discretize <= thresholds[1]] = 0
  variable.to.discretize[variable.to.discretize > thresholds[1] & variable.to.discretize < thresholds[2]] = 1
  variable.to.discretize[variable.to.discretize >= thresholds[2]] = 2
  return(variable.to.discretize)
}
set.seed(1)
test.data$age <- discretize.variable(test.data$age)
train.data$age <- discretize.variable(train.data$age)
hist(c(train.data$age, test.data$age))
```

Then we can fit the model
```{r message=FALSE, warning=FALSE}
fit.hierarchical <- brm(formula = DEATH_EVENT ~ ejection_fraction + serum_creatinine + serum_sodium + (ejection_fraction + serum_creatinine + serum_sodium| age),
           data = train.data,
           family = bernoulli(),
           c(set_prior("cauchy(0,5)", class="b", coef="serum_creatinine"),
                          set_prior("beta(6,4)", class="b", coef="ejection_fraction"),
                          set_prior("normal(100,40)", class="b", coef="serum_sodium")),
           refresh=0,
           control = list(adapt_delta = 0.99)
)
```


### Model for death time analysis  

As a fourth model we have model with response variable according to Weibull distribution, and features are selected based on correlation matrix.  
In death time analysis model we are using different functions based on whether sample is censored or not. If sample is censored, then we are using log probability density function  


$$Weibull(y|\alpha, \sigma) = \dfrac{\alpha}{\sigma}\left(\dfrac{y}{\sigma}\right)^{-\alpha-1}exp\left(-\left(\dfrac{y}{\sigma}\right)^{\alpha}\right)$$.


If sample is not censored, then we are using the cumulative function of previously mentioned probability density function.  
First we need to retrieve the original data with original age values

```{r}
set.seed(1)
new.data <- split.train.test(heart)
train.data <- new.data$train
test.data <- new.data$test
```

Then we can fit the model
```{r message=FALSE, warning=FALSE}
fit.weibull <- brm(formula = time | cens(DEATH_EVENT) ~ age  + anaemia + high_blood_pressure,
           data = train.data,
           family = weibull(),
           prior = c(set_prior("cauchy(40, 20)", class="b", coef="age"),
                     prior_string("normal(.5, .5)", class="b", coef="anaemia"),
                     prior_string("normal(.5, .5)", class="b", coef="high_blood_pressure")),
           refresh=0
)
```

## Prior choices 

We chose priors based on articles that we read about medical measurements. We used weakly informative priors, because there's only little indication in what type of prior we should use. 

### First models

Same priors were used more or less to every of first 3 models, and we will specify them here.  

**Coefficient distributions**  
- Age distribution is using half Cauchy distribution, which fits our population well. Cauchy is non-negative continuous distribution, which peaks at around 40, and decades to over 100. This is good, because it forces the prior to be non-negative, which is a must in age distribution. Also it centers around 40, which is convenient, because our age started from 40. 
$$age \sim Cauchy(y|\mu,\sigma) = \dfrac{1}{\pi\sigma}\dfrac{1}{1+((y-\mu)/\sigma)^2}$$
$$age \sim Cauchy(40,20)$$

- Serum creatinine used inverse gamma distribution. Creatinine levels also need non-negative distribution, and the normal level should be between 0.9 and 1.3. Severe symptoms start when creatinine reaches over 5, so we don't restrict the tail.
$$\text{serum creatinine} \sim InvGamma(y|\alpha, \beta) = \dfrac{\beta^{\alpha}}{\Gamma(\alpha)}y^{-(\alpha + 1)}exp\left(-\beta \dfrac{1}{y}\right)$$
$$\text{serum creatinine} \sim  InvGamma(1,1)$$

- Ejection fraction is constrained to be from 0 to 100, as its expressed in percentage. For this reason we need to use beta distribution. We chose to parametrize beta distribution as (6,4), because the ejection fraction should be between 50 and 70. (6,4) parametrization provides us mean of 0.6.
$$\text{ejection fraction} \sim Beta(y|\alpha, \beta) \dfrac{1}{B(\alpha, \beta)}\theta^{\alpha - 1}(1-\theta)^{\beta-1}$$
$$\text{ejection fraction} \sim Beta(6,4)$$

- Serum sodium should be around 135 and 145, but we gave it large variance with (half) cauchy(0,200). Severe symptoms of too high sodium levels start above 160, so this prior gives a lot room to reach fatal levels. 
$$\text{serum sodium} \sim Cauchy(y|\mu,\sigma) = \dfrac{1}{\pi\sigma}\dfrac{1}{1+((y-\mu)/\sigma)^2}$$
$$\text{serum sodium} \sim Cauchy(0,200)$$
- Creatinine phosphokinase regular levels are between 20 and 200, but it can reach in certain coditions even hundreds of thousand. This is why we dont want to limit our priors. We chose 

$$\text{serum sodium} \sim Cauchy(0,4000)$$

- Priors for anemia, smoking, high blood pressure and sex were chosen as uniform beta(1,1). This is because they might get values 0 or 1 with equal probabilities.  
$$\text{anemia, smoking, high blood pressure, sex} \sim \mathcal{N}(.5,.5)$$  


**Intercept distribution**
Intercept distribution was student_t for every coefficient. The student_t distribution is parameterized by default as student_t(3, 0, 2.5).
$$\text{Intercept} \sim StudentT(y|\upsilon, \mu, \sigma) = \dfrac{\Gamma((\upsilon+1)/2)}{\Gamma(\upsilon/2)}\left(1+\dfrac{1}{\upsilon}\left(\dfrac{y-\mu}{\sigma}\right)^2\right)^{-(\upsilon+1)/2}$$
$$\text{Intercept} \sim StudentT(3, 0, 2.5)$$

By default BRMS uses improper flat prior over the reals for population level parameters. Group level parameter is assumed to come from multivariate normal distribution with zero mean and unknown covariance matrix. 


There is more than one group-level effect per grouping factor, so correlations between those effects have to be estimated. Group level correlation matrix is generated uniformly over all positive definite correlation matrices using LKJ distribution as shown below:

$$
\text{LkjCholesky}(L|\eta)
\propto \left|J\right|\det(L L^\top)^{(\eta - 1)} = \prod_{k=2}^K
L_{kk}^{K-k+2\eta-2}
$$

## $\hat{R}$ convergence 

### Linear model with all variables 
```{r}
rhats <- rhat(fit.full)
color_scheme_set("brightblue")
mcmc_rhat(rhats) + yaxis_text(hjust = 1) + ggtitle("Rhat values for model with all variables") + theme(plot.title = element_text(hjust = 0.5))
```


### Linear model with selected variables 

```{r}
rhats <- rhat(fit.feature_selected)
color_scheme_set("brightblue")
mcmc_rhat(rhats) + yaxis_text(hjust = 1) + ggtitle("Rhat values for model with selected variables") + theme(plot.title = element_text(hjust = 0.5))
```

### Hierarchical model with all variables 
```{r}
rhats <- rhat(fit.hierarchical)
color_scheme_set("brightblue")
mcmc_rhat(rhats) + yaxis_text(hjust = 1) + ggtitle("Rhat values for hierarchical model with all variables") + theme(plot.title = element_text(hjust = 0.5))
```

### Weibull model

```{r}
rhats <- rhat(fit.weibull)
color_scheme_set("brightblue")
mcmc_rhat(rhats) + yaxis_text(hjust = 1) + ggtitle("Rhat values for Weibull model") + theme(plot.title = element_text(hjust = 0.5))
```

From the model summaries, it is observable that $\hat{R}$ values for all models are around 1.0 which is below the threshold value of 1.05 as mentioned in Stan documentation. With this information, we can interpret that the chains have mixed well and the samples are reliable.


## HMC specific convergence diagnostics (divergences, tree depth) with interpretation of the results

### Feature selected model

```{r}
check_divergences(fit.feature_selected$fit)
```

```{r}
check_treedepth(fit.feature_selected$fit)
```

### Full model 

```{r}
check_divergences(fit.full$fit)
```

```{r}
check_treedepth(fit.full$fit)
```

### Hierarchical model

```{r}
check_divergences(fit.hierarchical$fit)
```

```{r}
check_treedepth(fit.hierarchical$fit)
```

### Weibull model

```{r}
check_divergences(fit.weibull$fit)
```

```{r}
check_treedepth(fit.weibull$fit)
```

As we can see, we hardly observe any divergence. However we believe the ratio of the divergent iterations  and the total iterations can be considered negligible and maybe can be fixed using different priors (more informative). More accurate analysis about the models can be found in the next sections where we investigate more on the quality of the latters. 

## Effective sample size diagnostic (n_eff or ESS)

### Feature selected model
```{r}
s<-summary(fit.feature_selected)
s[["fixed"]][,6:7]
ratio<-neff_ratio(fit.feature_selected)
mcmc_neff(ratio)
```

### Full model
```{r}
s<-summary(fit.full)
s[["fixed"]][,6:7]
ratio<-neff_ratio(fit.full)
mcmc_neff(ratio)
```

### Hierarchical model
```{r}
s<-summary(fit.hierarchical)
s[["fixed"]][,6:7]
ratio<-neff_ratio(fit.hierarchical)
mcmc_neff(ratio)
```

### Weibull model
```{r}
s<-summary(fit.weibull)
s[["fixed"]][,6:7]
ratio<-neff_ratio(fit.weibull)
mcmc_neff(ratio)
```

The effective sample size (ESS) measures the amount by which autocorrelation in samples increases uncertainty (standard errors) relative to an independent sample. In other words the effective sample size is an estimate of the number of independent draws from the posterior distribution of the estimand of interest. The $n_{eff}$ metric used in Stan is based on the ability of the draws to estimate the true mean value of the parameter. Usually smaller than the total sample size $N$ so the larger the ratio to $N$, the better. As we can see from the plots, mostly all the ratios of effective sample size to total sample size can be conisdered good. For the feature selected model only one chain out of 4 has a ration below 0.5. Concerning the full model, we obtained that all the chain has a good $N_{eff}/N$ ratio but again we can find a chain whose ration is slighlty below 0.5. A similar conclusion can be derive from the weibull model. Regarding the hierarchical one, we can find that nearly 50% of the chains present a ratio below 0.5, and then further evaluation need to be carried out.

## Posterior predictive checking

First three models seem to converge well to the actual test set. The death event is more predictable than the survival because the events leading to death are more predictable.

Weibull model has a lot of variance but as the underlying distribution is multimodal, it is hard to modelled with small amount of data. The double peak is explained by the fact that some patients already had a high probability of death due to severe ventricular systolic dysfunction. The model does perform well with regards to detecting the death event at certain timestep. Due to the small amount of data, the distribution might be hard to interpret the actual scenario.

### Feature selected model

```{r}
pp_check(fit.feature_selected, nsamples=50, newdata = test.data, title="jou")
```

### Full model
```{r}
pp_check(fit.full, nsamples=50, newdata = test.data)
```

### Hierarchical model
```{r}
test.data$age <- discretize.variable(test.data$age)
train.data$age <- discretize.variable(train.data$age)
pp_check(fit.hierarchical, nsamples=50, newdata = test.data)
```

### Weibull model
```{r}
set.seed(1)
new.data <- split.train.test(heart)
train.data <- new.data$train
test.data <- new.data$test
pp_check(fit.weibull, nsamples=50, newdata = test.data)
```

\newpage
## Model comparison and interpretation of the results
A detailed comparison is carried at the end of the "Model comparison and interpretation of the results" section.

### Full model

```{r}
loo.full <-loo(fit.full)

hist(loo.full$diagnostics$pareto_k, main = "Diagnostic histogram of Pareto k",  xlab = "k-values", 
     ylab = "Frequency", freq = FALSE)
```

### Feature selected model

```{r}
loo.feature_selected <-loo(fit.feature_selected)

hist(loo.feature_selected$diagnostics$pareto_k, main = "Diagnostic histogram of Pareto k",  xlab = "k-values", 
     ylab = "Frequency", freq = FALSE)
```

### Hierarchical model
```{r}
loo.hierarchical <- loo(fit.hierarchical)

hist(loo.hierarchical$diagnostics$pareto_k, main = "Diagnostic histogram of Pareto k",  xlab = "k-values", 
     ylab = "Frequency",
     freq = FALSE)
```

### Weibull model

```{r}
loo.weibull <- loo(fit.weibull)

hist(loo.weibull$diagnostics$pareto_k, main = "Diagnostic histogram of Pareto k",  xlab = "k-values",
     ylab = "Frequency",
     freq = FALSE)
```

Model comparison: 

```{r}
loo_compare(list(loo.feature_selected, loo.full, loo.hierarchical))
```

Leave-one-out cross-validation is a method for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model using the log-likelihood evaluated at the posterior simulations of the parameter values [@loo].

From the PSIS_LOO we can see that the hierarchical model has the highest value compared to the other two models. This value is an indicator of model performance and the best model would be the one with the highest PSIS-LOO. Using the "loo" package we are able to estimate the difference in the models expected predictive accuracy and the function "loo_compare" creates a chart of the models with the one with highest ELPD (smallest LOOIC) first with zero values, meaning that is the best. We can see that the hierarchical model outstand the others, despite the difference is not huge.
Pareto k-values estimates the tail shape which determines the convergence rate of PSIS. K-values less than 0.7 are consdered to be a measure of reliability for the models in question. Values greater than 0.7 could represent a problem for the models, are further evaluation need to be done. As we can see from the k-values of the 4 models, none of the models return values below 0.7. We can state that the measure obtained using the loo package can be considere then reliable and the k-values help us to confirm our claim that the hierarhcical one is the best.

## Predictive performance assessment (classification)

For actually seeing how the models perform, we split the data into train and test sets. We have to use different train and test sets for the hierarchical model, but the pointwise accuracy assessment is straightforward in discrete models by checking the prediction classes with test data labels. 

Function for predicting pointwise accuracy for discrete models
```{r}
predict.pointwise.accuracy <- function(fitted.model, test.data) {
  preds <- round(predict(fitted.model, newdata = test.data)[,1])
  preds.correct <- preds == test.data$DEATH_EVENT

  pointwise.accuracy <- length(preds.correct[preds.correct == TRUE])/nrow(test.data)
  
  return(pointwise.accuracy)
}
```

For Weibull model, we can only find out the average days for death event to happen as it predicts the distribution for the number of days for death events. To assess its performance, predict function can be used. 

### Full model

```{r}
predict.pointwise.accuracy(fit.full, test.data)
```

### Feature selected model

```{r}
predict.pointwise.accuracy(fit.feature_selected, test.data)
```

### Hierarchical model

For hierarchical model, age column in the test data should be discretized since this feature is categorized to fit the model.
```{r}
discretized.test.data <- test.data
discretized.test.data$age <- discretize.variable(discretized.test.data$age)
predict.pointwise.accuracy(fit.hierarchical, discretized.test.data)
```

### Weibull model

For Weibull model, predict function is used to get the estimate values for the number of days it takes for a death event to happen since the model is continuous. 
```{r}
colMeans(predict(fit.weibull, test.data))
```

After the assessment, it seems that the hierarchical model performs better than the model with all variables and feature selected model when it comes to predict the correct labels (death or survival) for patients. 

## Prior sensitivity analysis (alternative prior tested)

We will use the model that has most priors for prior sensitivity analysis, the full model. This describes the meaning of prior well.

First let's use priors that we have chosen to be informative.
```{r message=FALSE, warning=FALSE}
prior.feature_selected.basic <- brm(formula = DEATH_EVENT ~ age + ejection_fraction + serum_creatinine + serum_sodium + high_blood_pressure + creatinine_phosphokinase + diabetes + smoking + anaemia + sex,                            
                                    data = train.data,
                            family = bernoulli(),
                          prior = c(set_prior("cauchy(40,20)", class="b", coef="age"),
                          set_prior("inv_gamma(1,5)", class="b", coef="serum_creatinine"),
                          set_prior("beta(6,4)", class="b", coef="ejection_fraction"),
                          set_prior("cauchy(0,4000)", class="b", coef="creatinine_phosphokinase"),
                          set_prior("cauchy(0,200)", class="b", coef="serum_sodium"),
                          set_prior("normal(.5, .5)", class="b", coef="anaemia"),
                          set_prior("normal(.5, .5)", class="b", coef="diabetes"),
                          set_prior("normal(.5, .5)", class="b", coef="smoking"),
                          set_prior("normal(.5, .5)", class="b", coef="high_blood_pressure"),
                          set_prior("normal(.5, .5)", class="b", coef="sex")),
                          refresh = 0,
                          control = list(adapt_delta = 0.99),
                          sample_prior = "only"
                            )
yrep <- brms::posterior_predict(prior.feature_selected.basic, newdata = test.data, draws=50)
yrep.new <- apply(yrep, 2, FUN = median)

df <- data.frame("yrep" = yrep.new, "test" = test.data$DEATH_EVENT)
ggplot(data=df) + geom_histogram(aes(y=yrep, fill="red", alpha=0.2)) + geom_histogram(aes(y=test, fill="blue", alpha=0.2))


plot(hist(yrep.new, plot = FALSE), col=c1)
plot(hist(test.data$DEATH_EVENT, plot = FALSE), col=c2, add=TRUE)


ppc_hist(y=test.data$DEATH_EVENT, yrep = yrep)
ppc_dens_overlay(y=test.data$DEATH_EVENT, yrep = yrep)

``` 

Then use uninformative normal prior
```{r message=FALSE, warning=FALSE}
prior.feature_selected.noninfo <- brm(formula = DEATH_EVENT ~ age + ejection_fraction + serum_creatinine + serum_sodium + high_blood_pressure + creatinine_phosphokinase + diabetes + smoking + anaemia + sex,
                            data = train.data,
                            family = bernoulli(),
                          prior = set_prior("normal(0,6000)"),
                          refresh = 0,
                          control = list(adapt_delta = 0.99),
                          sample_prior = "only"
                            )

yrep <- posterior_predict(prior.feature_selected.noninfo, newdata=test.data, draws=120)
ppc_dens_overlay(y=test.data$DEATH_EVENT, yrep = yrep)
```
As we see above, we are first fitting the models by ignoring likelihood. After that we generate samples based on test data, and see how well our model generates data fitting for the test data. We see, that by using our chosen priors we get better fit, which is expected as we are using more informative priors.



# Conclusion - Disucssion problems and further improvements

In this project we aimed to discover how different values of features for patients could affect the death event given by an heart failure. We explored how different models and different approaches affect the results of our objective. Our first discovery were that some variables had an higher correlation to the death event and we looked at this relationship when selecting the variables for our reduced model. The second model consist on a full feature model. The third model is a hierarchical in which we used the same variables we used in the first model, the substantial change was represented that now patients are divided into 3 groups based on their age. The last model was created by using a different approach to the dataset, we considered time as outcome variable with respect to death event and we tried to predict the time with respect to death event. We compared the 4 models using different assessment criteria (accuracy, loo, $\hat{R}$ and $n_{eff}$) and we can conclude that the hierarchical model performed the best. With the posterior performance assessment we can conclude that the model fairly good and we would recommended this model as an accurate predictor
Future improvement could be looking at more advanced future engineering and differnt choice of priors. 

## Self-reflection

We learned to work as a group. We used GitHub for version management, and it was nice to see that with communication we had zero conflicts, even though we only worked on main-branch. Seems like we also stayed on time perfectly. At first we were having group meetings really early, but it was worth it because we had longer time to think about the problem and possible solutions. 

\newpage
# Appendix {-}

## A. Stan code of full model {-}
```{r}
stancode(fit.full)
```

\newpage
## B. Stan feature selected model {-}
```{r}
stancode(fit.feature_selected)
```

\newpage
## C. Stan hierarhical model {-}
```{r}
stancode(fit.hierarchical)
```

\newpage
## D. Stan death time analysis model
```{r}
stancode(fit.weibull)
```

\newpage
# References
